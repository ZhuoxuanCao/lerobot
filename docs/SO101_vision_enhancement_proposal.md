# SO-101æœºå™¨äººæ“ä½œç­–ç•¥ä¼˜åŒ–æ–¹æ¡ˆ

**ç»„ä¼šæŠ¥å‘Šæ–‡æ¡£**

---

## ğŸ“‹ ä¸€ã€é¡¹ç›®èƒŒæ™¯ä¸ç°çŠ¶

### 1.1 å½“å‰ä»»åŠ¡æè¿°

æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ **SO-101 å…­è‡ªç”±åº¦æœºæ¢°è‡‚** é…åˆ **æ‰‹çœ¼ç›¸æœºï¼ˆeye-in-handï¼‰** å®Œæˆå †å ä»»åŠ¡ï¼š
1. **å­ä»»åŠ¡1**: æŠ“å–ç»¿è‰²æ–¹å—ï¼Œæ”¾å…¥å·¦ä¾§é»‘è‰²æ¡†ä¸­
2. **å­ä»»åŠ¡2**: æŠ“å–è“è‰²æ–¹å—ï¼Œç æ”¾åœ¨ç»¿è‰²æ–¹å—ä¹‹ä¸Š

**è®­ç»ƒæ–¹æ³•**: åŸºäº HuggingFace LeRobot æ¡†æ¶çš„ **ACT (Action Chunking with Transformers)** æ¨¡ä»¿å­¦ä¹ ç­–ç•¥

### 1.2 è§‚æµ‹åˆ°çš„æ ¸å¿ƒé—®é¢˜

ç»è¿‡ v2ã€v3ã€v6 å¤šä¸ªç‰ˆæœ¬æ•°æ®é›†çš„è®­ç»ƒï¼ˆå…±55ä¸ªepisodeï¼Œ9732å¸§ï¼‰ï¼Œæˆ‘ä»¬å‘ç°ä¸‰ä¸ªç³»ç»Ÿæ€§é—®é¢˜ï¼š

| é—®é¢˜ç±»å‹ | å…·ä½“è¡¨ç° | ä¸¥é‡ç¨‹åº¦ |
|---------|---------|---------|
| **ç³»ç»Ÿæ€§æ”¾ç½®è¯¯å·®** | ç»¿è‰²æ–¹å—çš„æ”¾ç½®ä½ç½®**ç¨³å®šåœ°**åå‘ç›®æ ‡ç‚¹çš„å·¦ä¸‹æ–¹ 3-5cm | ğŸ”´ é«˜ |
| **æŠ“å–å¤±è´¥ç‡é«˜** | æŠ“å–è“è‰²æ–¹å—æ—¶æˆåŠŸç‡ä½ï¼ˆ~40%ï¼‰ï¼Œç»å¸¸æŠ“ç©º | ğŸŸ¡ ä¸­ |
| **ç å›ç¨³å®šæ€§å·®** | è“è‰²æ–¹å—æ”¾ç½®åœ¨ç»¿è‰²æ–¹å—ä¸Šæ—¶å®¹æ˜“æ»‘è½ | ğŸŸ¡ ä¸­ |

**å…³é”®æ´å¯Ÿ**:
- "ç¨³å®šåç§»" â‰  éšæœºè¯¯å·® â†’ æç¤ºæ¨¡å‹å­¦ä¹ åˆ°äº†**é”™è¯¯çš„ç³»ç»Ÿæ€§æ¨¡å¼**
- æ€€ç–‘æ¨¡å‹è¿‡åº¦ä¾èµ–**æœ¬ä½“æ„Ÿå®˜ï¼ˆå…³èŠ‚è§’åº¦ï¼‰**ï¼Œè€Œé**è§†è§‰ä¿¡æ¯ï¼ˆç›¸æœºå›¾åƒï¼‰**

---

## ğŸ”¬ äºŒã€é—®é¢˜æ ¹å› åˆ†æ

### 2.1 ACT æ¨¡å‹æ¶æ„å›é¡¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è¾“å…¥å±‚                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ è§†è§‰è¾“å…¥: 2ä¸ªç›¸æœºå›¾åƒ â†’ ResNet18ç‰¹å¾æå–          â”‚
â”‚ â€¢ æœ¬ä½“æ„Ÿå®˜: 6ä¸ªå…³èŠ‚è§’åº¦ (robot_state)               â”‚
â”‚ â€¢ VAEæ½œå˜é‡: ç¼–ç åŠ¨ä½œåºåˆ—çš„éšæœºæ€§                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Transformer Encoder                  â”‚
â”‚  èåˆè§†è§‰ç‰¹å¾ + æœ¬ä½“çŠ¶æ€ + æ½œå˜é‡                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Transformer Decoder                  â”‚
â”‚  é€šè¿‡"äº¤å‰æ³¨æ„åŠ›"æœºåˆ¶æŸ¥è¯¢encoderè¾“å‡º                 â”‚
â”‚  ğŸ”¥ å…³é”®ï¼šå†³å®š"å…³æ³¨è§†è§‰"è¿˜æ˜¯"å…³æ³¨çŠ¶æ€"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
                 é¢„æµ‹åŠ¨ä½œåºåˆ— (chunk_size=100)
```

### 2.2 æ·å¾„å­¦ä¹ ï¼ˆShortcut Learningï¼‰å‡è¯´

**å‡è®¾**: æ¨¡å‹å­¦åˆ°äº†"æ·å¾„è§„åˆ™"è€ŒéçœŸæ­£çš„å› æœå…³ç³»

#### é”™è¯¯å­¦ä¹ æ¨¡å¼ï¼ˆæ·å¾„ï¼‰
```python
# æ¨¡å‹å†…éšå­¦ä¹ åˆ°çš„è§„åˆ™
if shoulder_pan_angle == -30Â° and elbow_flex == 45Â°:
    gripper.open()  # æ¾æ‰‹æ”¾ä¸‹æ–¹å—
```

**é—®é¢˜**: è¿™ä¸ªè§„åˆ™åœ¨è®­ç»ƒé›†ä¸­workï¼ˆå› ä¸ºç¤ºæ•™è€…æ¯æ¬¡ä½ç½®ç›¸ä¼¼ï¼‰ï¼Œä½†**æ³›åŒ–èƒ½åŠ›ä¸ºé›¶**
- å¦‚æœç‰©ä½“ä½ç½®ç¨æœ‰åç§» â†’ æœºæ¢°è‡‚ä»åœ¨"é”™è¯¯ä½ç½®"æ¾æ‰‹ â†’ ç³»ç»Ÿæ€§åå·®

#### æ­£ç¡®å­¦ä¹ æ¨¡å¼ï¼ˆæœŸæœ›ï¼‰
```python
# æœŸæœ›æ¨¡å‹å­¦ä¹ çš„è§„åˆ™
if vision.detect("ç»¿è‰²æ–¹å—åœ¨é»‘æ¡†ä¸­å¿ƒ"):
    gripper.open()  # åœ¨è§†è§‰ç¡®è®¤åæ¾æ‰‹
```

**ä¼˜åŠ¿**: åŸºäºè§†è§‰åé¦ˆï¼Œå…·æœ‰æ³›åŒ–èƒ½åŠ›

### 2.3 è¯Šæ–­ä¾æ®ï¼šæ³¨æ„åŠ›æœºåˆ¶åˆ†æ

Transformer çš„**äº¤å‰æ³¨æ„åŠ›æƒé‡** å¯ä»¥æ­ç¤ºæ¨¡å‹"å…³æ³¨ä»€ä¹ˆ"ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

å…¶ä¸­æ³¨æ„åŠ›æƒé‡ $\text{softmax}(QK^T/\sqrt{d_k})$ çš„æ¯ä¸ªå…ƒç´ è¡¨ç¤ºï¼š
- **ç¬¬ i ä¸ªé¢„æµ‹åŠ¨ä½œ** å¯¹ **ç¬¬ j ä¸ªè¾“å…¥token** çš„å…³æ³¨åº¦

**è¾“å…¥tokensæ„æˆ**:
```
[latent(1), robot_state(1), image_feature_map(HÃ—WÃ—2ç›¸æœº) ]
  ç´¢å¼•0        ç´¢å¼•1           ç´¢å¼•2 ~ 99
```

**é¢„æœŸå¥åº·æ¯”ä¾‹** (è§†è§‰ä¸ºä¸»ä»»åŠ¡):
- è§†è§‰æ³¨æ„åŠ›: **60-80%**
- çŠ¶æ€æ³¨æ„åŠ›: **20-40%**

**å®é™…æµ‹é‡ç»“æœ** (æ¨æµ‹ï¼Œéœ€éªŒè¯):
- è§†è§‰æ³¨æ„åŠ›: **~25%** âš ï¸
- çŠ¶æ€æ³¨æ„åŠ›: **~75%** âš ï¸

**ç»“è®º**: æ¨¡å‹ä¸¥é‡ä¾èµ–æœ¬ä½“æ„Ÿå®˜ï¼Œè§†è§‰ä¿¡æ¯åˆ©ç”¨ä¸è¶³

---

## ğŸ’¡ ä¸‰ã€æå‡ºçš„è§£å†³æ–¹æ¡ˆ

### 3.1 æ–¹æ¡ˆæ¦‚è§ˆ

æˆ‘ä»¬è®¡åˆ’å®æ–½ **ä¸¤é¡¹äº’è¡¥æŠ€æœ¯** æ¥é‡æ–°å¹³è¡¡æ¨¡å‹å¯¹è§†è§‰å’ŒçŠ¶æ€çš„ä¾èµ–ï¼š

| æ–¹æ³• | ç±»å‹ | ä½œç”¨å±‚ | æœºåˆ¶ |
|------|------|--------|------|
| **æ–¹æ³•1: æ³¨æ„åŠ›æŸå¤±** | ä¸»åŠ¨æ­£åˆ™åŒ– | Transformeræ³¨æ„åŠ›å±‚ | æ•°å­¦çº¦æŸï¼Œæƒ©ç½šè¿‡åº¦å…³æ³¨çŠ¶æ€ |
| **æ–¹æ³•3: æ¨¡æ€ä¸¢å¼ƒ** | è¾“å…¥æ‰°åŠ¨ | æ•°æ®è¾“å…¥å±‚ | éšæœºå±è”½çŠ¶æ€ï¼Œå¼ºåˆ¶å­¦ä¹ è§†è§‰ |

> **ä¸ºä»€ä¹ˆä¸ç”¨æ–¹æ³•2ï¼ˆROIè£å‰ªï¼‰ï¼Ÿ**
> ROIè£å‰ªéœ€è¦é¢å¤–çš„ç‰©ä½“æ£€æµ‹æ¨¡å—ï¼Œå¢åŠ ç³»ç»Ÿå¤æ‚åº¦ã€‚ä½œä¸ºç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬ä¸“æ³¨äºçº¯ç®—æ³•æ”¹è¿›ã€‚

---

### 3.2 æ–¹æ³•1ï¼šæ³¨æ„åŠ›æŸå¤±ï¼ˆAttention Regularization Lossï¼‰

#### åŸç†æ¦‚è¿°

åœ¨åŸæœ‰çš„ä»»åŠ¡æŸå¤±ï¼ˆL1 lossï¼‰åŸºç¡€ä¸Šï¼Œæ·»åŠ ä¸€ä¸ª**æ­£åˆ™åŒ–é¡¹**æ¥å¼•å¯¼æ¨¡å‹çš„æ³¨æ„åŠ›åˆ†å¸ƒï¼š

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{L1}} + \lambda \cdot \mathcal{L}_{\text{attention}}
$$

**å‚æ•°è¯´æ˜**ï¼š

- **æ€»æŸå¤±** $\mathcal{L}_{\text{total}}$ï¼šæœ€ç»ˆçš„æ€»æŸå¤±ï¼Œç”¨äºåå‘ä¼ æ’­æ›´æ–°æ¨¡å‹å‚æ•°

- **ä»»åŠ¡æŸå¤±** $\mathcal{L}_{\text{L1}}$ï¼šåŸæœ‰çš„ä»»åŠ¡æŸå¤±ï¼Œè¡¡é‡é¢„æµ‹åŠ¨ä½œä¸çœŸå®åŠ¨ä½œçš„å·®è·

  - è®¡ç®—æ–¹å¼ï¼š

    $$
    \mathcal{L}_{\text{L1}} = \frac{1}{N}\sum_{i=1}^{N} |a_i^{\text{pred}} - a_i^{\text{true}}|
    $$

  - å…¶ä¸­ $a_i$ ä¸ºç¬¬ $i$ ä¸ªåŠ¨ä½œï¼Œ$N$ ä¸ºåŠ¨ä½œåºåˆ—é•¿åº¦

  - è¿™æ˜¯æ¨¡å‹å­¦ä¹ ä»»åŠ¡æœ¬èº«çš„æ ¸å¿ƒæŸå¤±ï¼Œ**å¿…é¡»ä¿æŒ**

- **æƒé‡ç³»æ•°** $\lambda$ï¼šå¹³è¡¡ä»»åŠ¡å­¦ä¹ å’Œæ³¨æ„åŠ›æ­£åˆ™åŒ–çš„è¶…å‚æ•°

  - æ¨èå€¼ï¼š$\lambda = 0.01$
  - å«ä¹‰ï¼šæ³¨æ„åŠ›æŸå¤±åœ¨æ€»æŸå¤±ä¸­çš„ç›¸å¯¹é‡è¦æ€§
  - è¿‡å°ï¼ˆå¦‚ 0.001ï¼‰â†’ æ­£åˆ™åŒ–æ•ˆæœå¼±ï¼Œå¯èƒ½æ— æ•ˆ
  - è¿‡å¤§ï¼ˆå¦‚ 0.1ï¼‰â†’ å¯èƒ½å¹²æ‰°ä»»åŠ¡å­¦ä¹ ï¼Œå¯¼è‡´ä¸æ”¶æ•›

- **æ³¨æ„åŠ›æ­£åˆ™åŒ–æŸå¤±** $\mathcal{L}_{\text{attention}}$ï¼šä¸‹æ–‡è¯¦ç»†å±•å¼€

---

#### æ³¨æ„åŠ›æŸå¤±çš„ç»„æˆ

æ³¨æ„åŠ›æŸå¤±ç”±**ä¸¤ä¸ªäº’è¡¥çš„å­ç›®æ ‡**ç»„æˆï¼Œå…±åŒå¼•å¯¼æ¨¡å‹å½¢æˆç†æƒ³çš„æ³¨æ„åŠ›æ¨¡å¼ï¼š

$$
\mathcal{L}_{\text{attention}} = \mathcal{L}_{\text{ratio}} + \alpha \cdot \mathcal{L}_{\text{entropy}}
$$

å…¶ä¸­ $\alpha$ ä¸ºä¸¤ä¸ªå­ç›®æ ‡çš„å¹³è¡¡ç³»æ•°ï¼ˆæ¨èå€¼ 0.5ï¼‰ã€‚

---

#### å­ç›®æ ‡1: æ¯”ä¾‹æƒ©ç½šæŸå¤±ï¼ˆRatio Penalty Lossï¼‰

**æ•°å­¦è¡¨è¾¾å¼**ï¼š

$$
\mathcal{L}_{\text{ratio}} = \text{ReLU}\left(\frac{\sum_{i,j \in \text{state}} w_{ij}}{\sum_{i,j} w_{ij}} - \tau\right)
$$

**ç¬¦å·è¯¦è§£**ï¼š

| ç¬¦å· | å«ä¹‰ | ç»´åº¦/å–å€¼ |
|------|------|----------|
| $w_{ij}$ | æ³¨æ„åŠ›æƒé‡çŸ©é˜µçš„å…ƒç´  | [0, 1]ï¼Œæ¥è‡ª softmax |
| $i$ | åŠ¨ä½œåºåˆ—ç´¢å¼• | $i = 1, 2, ..., C$ (`chunk_size`) |
| $j$ | encoder token ç´¢å¼• | $j = 1, 2, ..., S$ (sequence length) |
| $\sum_{\text{state}} w_{ij}$ | **çŠ¶æ€ tokens çš„æ€»æ³¨æ„åŠ›** | ç´¯åŠ  $j \in \{0, 1\}$ çš„æƒé‡ |
| $\sum_{\text{all}} w_{ij}$ | **æ‰€æœ‰ tokens çš„æ€»æ³¨æ„åŠ›** | å½’ä¸€åŒ–åŸºå‡† |
| $\tau$ (tau) | **ç›®æ ‡é˜ˆå€¼**ï¼ˆè¶…å‚æ•°ï¼‰ | æ¨èå€¼ $\tau = 0.3$ (30%) |
| $\text{ReLU}(\cdot)$ | ä¿®æ­£çº¿æ€§å•å…ƒ | $\max(0, \cdot)$ï¼Œåªæƒ©ç½šè¿è§„æƒ…å†µ |

**å·¥ä½œæœºåˆ¶**ï¼š

**æ­¥éª¤1ï¼šè®¡ç®—çŠ¶æ€æ³¨æ„åŠ›å æ¯”**

$$
r_{\text{state}} = \frac{\sum_{i,j \in \text{state}} w_{ij}}{\sum_{i,j} w_{ij}}
$$

- åˆ†å­ï¼šæ¨¡å‹åˆ†é…ç»™ `robot_state` å’Œ `latent` çš„æ€»æ³¨æ„åŠ›
- åˆ†æ¯ï¼šæ¨¡å‹åˆ†é…ç»™æ‰€æœ‰è¾“å…¥ï¼ˆçŠ¶æ€+è§†è§‰ï¼‰çš„æ€»æ³¨æ„åŠ›
- ç»“æœï¼šçŠ¶æ€å æ¯”ï¼ŒèŒƒå›´ $[0, 1]$

**æ­¥éª¤2ï¼šåˆ¤æ–­æ˜¯å¦è¶…è¿‡é˜ˆå€¼**

$$
\text{è¿è§„é‡} = r_{\text{state}} - \tau
$$

- å¦‚æœ $r_{\text{state}} = 0.25 < 0.3$ï¼Œåˆ™è¿è§„é‡ = -0.05ï¼Œ**ç¬¦åˆæœŸæœ›ï¼Œæ— æƒ©ç½š**
- å¦‚æœ $r_{\text{state}} = 0.5 > 0.3$ï¼Œåˆ™è¿è§„é‡ = +0.2ï¼Œ**è¶…å‡ºé˜ˆå€¼ï¼Œäº§ç”Ÿæƒ©ç½š**

**æ­¥éª¤3ï¼šåº”ç”¨ ReLU æƒ©ç½š**

$$
\mathcal{L}_{\text{ratio}} = \max(0, r_{\text{state}} - 0.3)
$$

- ReLU çš„ä½œç”¨ï¼š**å•å‘æƒ©ç½š**
- åªåœ¨çŠ¶æ€å æ¯”è¿‡é«˜æ—¶äº§ç”ŸæŸå¤±
- å¦‚æœçŠ¶æ€å æ¯”å·²ç»å¾ˆä½ï¼ˆå¦‚ 10%ï¼‰ï¼Œä¸ä¼šå¼ºåˆ¶æé«˜

**ç›´è§‚ç†è§£**ï¼š

```python
# ä¼ªä»£ç æ¼”ç¤º
if çŠ¶æ€æ³¨æ„åŠ›å æ¯” > 30%:
    æŸå¤± = (çŠ¶æ€æ³¨æ„åŠ›å æ¯” - 30%) * æ¢¯åº¦æƒé‡
    # æ¢¯åº¦ä¼šä¿ƒä½¿æ¨¡å‹å‡å°‘å¯¹çŠ¶æ€çš„å…³æ³¨
else:
    æŸå¤± = 0  # å·²ç»è¾¾æ ‡ï¼Œä¸æ–½åŠ é¢å¤–çº¦æŸ
```

**ä¸ºä»€ä¹ˆé˜ˆå€¼æ˜¯ 0.3ï¼Ÿ**

- **ä»»åŠ¡ç‰¹æ€§**ï¼šè§†è§‰ä¸ºä¸»çš„æ“ä½œä»»åŠ¡ï¼Œç†æƒ³æ¯”ä¾‹ä¸º 70% è§†è§‰ + 30% çŠ¶æ€
- **é€‚åº¦ä¾èµ–**ï¼šç²¾ç»†æ“ä½œç¡®å®éœ€è¦æœ¬ä½“æ„Ÿå®˜åé¦ˆï¼ˆå¦‚åŠ›æ§ï¼‰
- **ç»éªŒå€¼**ï¼šåœ¨ NLP å’Œè§†è§‰é¢†åŸŸçš„ç±»ä¼¼æ­£åˆ™åŒ–ä¸­ï¼Œ30-40% æ˜¯å¸¸è§å¹³è¡¡ç‚¹

---

#### å­ç›®æ ‡2: ç†µæœ€å¤§åŒ–æŸå¤±ï¼ˆEntropy Maximization Lossï¼‰

**æ•°å­¦è¡¨è¾¾å¼**ï¼š

$$
\mathcal{L}_{\text{entropy}} = -\frac{1}{C} \sum_{i=1}^{C} H(w_i^{\text{vision}})
$$

å…¶ä¸­å•ä¸ªåŠ¨ä½œçš„ç†µå®šä¹‰ä¸ºï¼š

$$
H(w_i^{\text{vision}}) = -\sum_{j \in \text{vision}} p_{ij} \log p_{ij}
$$

**ç¬¦å·è¯¦è§£**ï¼š

| ç¬¦å· | å«ä¹‰ | ç»´åº¦/å–å€¼ |
|------|------|----------|
| $C$ | åŠ¨ä½œåºåˆ—é•¿åº¦ | `chunk_size = 100` |
| $i$ | ç¬¬ $i$ ä¸ªé¢„æµ‹åŠ¨ä½œ | $i = 1, \ldots, 100$ |
| $w_i^{\text{vision}}$ | ç¬¬ $i$ ä¸ªåŠ¨ä½œå¯¹**è§†è§‰ tokens** çš„æ³¨æ„åŠ›åˆ†å¸ƒ | å‘é‡ï¼Œé•¿åº¦ = HÃ—WÃ—`n_cameras` |
| $p_{ij}$ | å½’ä¸€åŒ–çš„æ³¨æ„åŠ›æƒé‡ | $p_{ij} = \frac{w_{ij}}{\sum_{j \in \text{vision}} w_{ij}}$ |
| $H(\cdot)$ | ä¿¡æ¯ç†µ | è¡¡é‡åˆ†å¸ƒçš„"åˆ†æ•£ç¨‹åº¦" |
| $\log$ | è‡ªç„¶å¯¹æ•° | é€šå¸¸ä¸º $\log_e$ æˆ– $\log_2$ |

**å·¥ä½œæœºåˆ¶**ï¼š

**æ­¥éª¤1ï¼šå½’ä¸€åŒ–è§†è§‰æ³¨æ„åŠ›**

å¯¹äºç¬¬ $i$ ä¸ªåŠ¨ä½œï¼Œå°†å…¶å¯¹æ‰€æœ‰è§†è§‰ tokens çš„æ³¨æ„åŠ›å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼š

$$
p_{ij} = \frac{w_{ij}}{\sum_{j=2}^{S} w_{ij}}, \quad j \in \{2, 3, \ldots, S\}
$$

**æ­¥éª¤2ï¼šè®¡ç®—ç†µ**

$$
H(w_i^{\text{vision}}) = -\sum_{j=2}^{S} p_{ij} \log p_{ij}
$$

**ç†µçš„ç‰©ç†æ„ä¹‰**ï¼š

- **é«˜ç†µï¼ˆå¦‚ H=5.0ï¼‰**ï¼šæ³¨æ„åŠ›å‡åŒ€åˆ†æ•£åœ¨å¤šä¸ªè§†è§‰åŒºåŸŸ
  - ä¾‹å­ï¼š$p = [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]$ (10ä¸ªtoken)
  - è§£é‡Šï¼šæ¨¡å‹åœ¨"æ‰«è§†"æ•´ä¸ªåœºæ™¯ï¼Œå…³æ³¨å¤šä¸ªå…³é”®ç‚¹

- **ä½ç†µï¼ˆå¦‚ H=0.5ï¼‰**ï¼šæ³¨æ„åŠ›é›†ä¸­åœ¨å°‘æ•°å‡ ä¸ªtoken
  - ä¾‹å­ï¼š$p = [0.9, 0.05, 0.05, 0, 0, \ldots]$
  - è§£é‡Šï¼šæ¨¡å‹åªç›¯ç€ä¸€ä¸ªä½ç½®ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆäº†æŸä¸ªå›ºå®šç‰¹å¾

**æ­¥éª¤3ï¼šå¹³å‡æ‰€æœ‰åŠ¨ä½œçš„ç†µ**

$$
\text{å¹³å‡ç†µ} = \frac{1}{100} \sum_{i=1}^{100} H(w_i^{\text{vision}})
$$

**æ­¥éª¤4ï¼šæ„é€ æŸå¤±ï¼ˆè´Ÿç†µï¼‰**

$$
\mathcal{L}_{\text{entropy}} = -\text{å¹³å‡ç†µ}
$$

- **æœ€å°åŒ–è´Ÿç†µ = æœ€å¤§åŒ–ç†µ**
- é¼“åŠ±æ¨¡å‹åˆ†æ•£æ³¨æ„åŠ›ï¼Œè€Œéå›ºå®šæŸä¸ªåŒºåŸŸ

**ç›´è§‚ç†è§£**ï¼š

```python
# æ¡ˆä¾‹å¯¹æ¯”
# ä¸è‰¯æ³¨æ„åŠ›æ¨¡å¼ï¼ˆä½ç†µ H=0.69ï¼‰
attention_bad = [0.8, 0.1, 0.05, 0.03, 0.02]  # 80%é›†ä¸­åœ¨ç¬¬ä¸€ä¸ªtoken
entropy_bad = -(0.8*log(0.8) + 0.1*log(0.1) + ...) = 0.69

# è‰¯å¥½æ³¨æ„åŠ›æ¨¡å¼ï¼ˆé«˜ç†µ H=1.61ï¼‰
attention_good = [0.2, 0.2, 0.2, 0.2, 0.2]  # å‡åŒ€åˆ†å¸ƒ
entropy_good = -(5 * 0.2*log(0.2)) = 1.61

# æˆ‘ä»¬çš„æŸå¤±ä¼šæƒ©ç½šä½ç†µï¼Œé¼“åŠ±é«˜ç†µ
loss_bad = -0.69 = -0.69  # æŸå¤±é«˜
loss_good = -1.61 = -1.61  # æŸå¤±ä½ï¼ˆæ³¨ï¼šæ¢¯åº¦æ–¹å‘ä½¿å…¶å¢å¤§ï¼‰
```

**ä¸ºä»€ä¹ˆéœ€è¦ç†µæœ€å¤§åŒ–ï¼Ÿ**

å‡è®¾æ²¡æœ‰ç†µçº¦æŸï¼Œæ¨¡å‹å¯èƒ½è¿™æ ·"ä½œå¼Š"ï¼š

- çŠ¶æ€æ³¨æ„åŠ›ï¼š20%ï¼ˆæ»¡è¶³æ¯”ä¾‹çº¦æŸï¼‰
- è§†è§‰æ³¨æ„åŠ›ï¼š80%ï¼Œä½†**å…¨éƒ¨é›†ä¸­åœ¨å›¾åƒä¸­å¿ƒä¸€ä¸ªåƒç´ **

è¿™ç§æƒ…å†µä¸‹ï¼š

- âœ… é€šè¿‡äº†æ¯”ä¾‹æ£€æŸ¥ï¼ˆ80% > 70%ï¼‰
- âŒ ä½†è§†è§‰ä¿¡æ¯åˆ©ç”¨ä»ç„¶å¾ˆå·®ï¼ˆåªçœ‹ä¸€ä¸ªç‚¹ï¼‰

å¼•å…¥ç†µçº¦æŸåï¼Œå¼ºåˆ¶æ¨¡å‹ï¼š

- âœ… ä¸ä»…è¦"å¤šçœ‹è§†è§‰"ï¼ˆæ¯”ä¾‹é«˜ï¼‰
- âœ… è¿˜è¦"çœ‹å¾—å…¨é¢"ï¼ˆç†µå¤§ï¼‰

---

#### ç»„åˆæŸå¤±çš„æœ€ç»ˆå½¢å¼

å°†ä¸¤ä¸ªå­ç›®æ ‡ç»„åˆï¼š

$$
\mathcal{L}_{\text{attention}} = \underbrace{\text{ReLU}\left(r_{\text{state}} - 0.3\right)}_{\text{æ¯”ä¾‹çº¦æŸ}} + \underbrace{0.5 \cdot \left(-\frac{1}{C}\sum_{i=1}^{C} H(w_i^{\text{vision}})\right)}_{\text{ç†µæ­£åˆ™åŒ–}}
$$

**ç³»æ•° 0.5 çš„ä½œç”¨**ï¼š

- å¹³è¡¡ä¸¤ä¸ªå­ç›®æ ‡çš„å°ºåº¦
- æ¯”ä¾‹æŸå¤±çš„å€¼åŸŸï¼š$[0, 0.7]$ (æœ€åæƒ…å†µï¼šçŠ¶æ€å æ¯”100%)
- ç†µæŸå¤±çš„å€¼åŸŸï¼š$[0, -\log(H \times W)]$ (å–å†³äºå›¾åƒåˆ†è¾¨ç‡)
- ç³»æ•° 0.5 ä½¿ä¸¤è€…è´¡çŒ®åº¦æ¥è¿‘

**æœ€ç»ˆæ¢¯åº¦æ–¹å‘**ï¼š

å½“åå‘ä¼ æ’­æ—¶ï¼Œæ¢¯åº¦ä¼šåŒæ—¶ä½œç”¨äºï¼š

1. **å‡å°‘çŠ¶æ€æ³¨æ„åŠ›æƒé‡** $w_{ij}^{\text{state}}$ï¼ˆå¦‚æœ $r_{\text{state}} > 0.3$ï¼‰
2. **å¢åŠ è§†è§‰æ³¨æ„åŠ›çš„åˆ†æ•£åº¦**ï¼ˆé€šè¿‡ç†µæ¢¯åº¦ï¼‰

**è®­ç»ƒåŠ¨æ€è¿‡ç¨‹**ï¼š

```
åˆå§‹çŠ¶æ€ (step 0):
  state_ratio = 0.75, vision_entropy = 1.2
  L_ratio = 0.45, L_entropy = -1.2
  L_attention = 0.45 + 0.5*(-1.2) = -0.15

ä¸­æœŸ (step 5000):
  state_ratio = 0.50, vision_entropy = 2.5
  L_ratio = 0.20, L_entropy = -2.5
  L_attention = 0.20 + 0.5*(-2.5) = -1.05

æ”¶æ•› (step 15000):
  state_ratio = 0.28, vision_entropy = 3.8
  L_ratio = 0.00, L_entropy = -3.8
  L_attention = 0.00 + 0.5*(-3.8) = -1.90
```

æ³¨æ„ï¼šç†µæŸå¤±ä¸ºè´Ÿå€¼æ˜¯æ­£å¸¸çš„ï¼Œæ¢¯åº¦ä¼˜åŒ–å™¨ä¼šå¤„ç†ç¬¦å·ã€‚

---

#### å®æ–½ç»†èŠ‚

**ä¿®æ”¹ä½ç½®**: `ACTPolicy.forward()` æ–¹æ³•
```python
# ä¼ªä»£ç 
loss = l1_loss  # åŸæœ‰ä»»åŠ¡æŸå¤±

if config.use_attention_loss:
    # ä»decoderå„å±‚æ”¶é›†æ³¨æ„åŠ›æƒé‡
    attn_weights = collect_attention_from_decoder_layers()

    # è®¡ç®—æ­£åˆ™åŒ–æŸå¤±
    attn_loss = compute_attention_regularization(attn_weights)

    # åŠ æƒå åŠ 
    loss = loss + 0.01 * attn_loss  # Î»=0.01
```

**è¶…å‚æ•°**:
- `Î» (attention_loss_weight)`: **0.01** (å¯è°ƒ)
- `target_vision_ratio`: **0.7** (æœŸæœ›è§†è§‰å æ¯”70%)

#### ç†è®ºæ”¯æ’‘

- **"Attention is All You Need"** (Vaswani et al., 2017): Transformeræ³¨æ„åŠ›çš„å¯è§£é‡Šæ€§
- **"Regularizing Attention"** (ICLR 2019): æ³¨æ„åŠ›æ­£åˆ™åŒ–æå‡æ³›åŒ–èƒ½åŠ›
- **"Shortcut Learning in DNNs"** (Geirhos et al., 2020): å¯¹æŠ—æ·å¾„å­¦ä¹ çš„ç†è®ºåŸºç¡€

---

### 3.3 æ–¹æ³•3ï¼šæ¨¡æ€ä¸¢å¼ƒï¼ˆModality Dropoutï¼‰

#### åŸç†

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œ**éšæœº**ï¼ˆæ¦‚ç‡ p=0.3ï¼‰å°†æœºå™¨äººçŠ¶æ€ `robot_state` ç½®é›¶ï¼š

```python
# è®­ç»ƒæ—¶ï¼ˆ30%æ¦‚ç‡è§¦å‘ï¼‰
if training and random() < 0.3:
    batch["observation.state"] = zeros_like(...)

# æ¨ç†æ—¶ï¼ˆä¸è§¦å‘ï¼Œæ­£å¸¸ä½¿ç”¨çŠ¶æ€ï¼‰
```

**æ•ˆæœ**:
- æ¨¡å‹è¢«è¿«åœ¨"ç¼ºå°‘çŠ¶æ€ä¿¡æ¯"çš„æƒ…å†µä¸‹ä¹Ÿèƒ½å·¥ä½œ
- åªèƒ½ä¾èµ–è§†è§‰æ¥é¢„æµ‹åŠ¨ä½œ
- æ¨ç†æ—¶è™½ç„¶æœ‰çŠ¶æ€ï¼Œä½†æ¨¡å‹å·²ç»"ä¹ æƒ¯"ä¸»è¦çœ‹è§†è§‰

#### å®æ–½ç»†èŠ‚

**ä¿®æ”¹ä½ç½®**: `ACT.forward()` æ–¹æ³•å¼€å¤´
```python
def forward(self, batch):
    # ğŸ”¥ æ–°å¢ï¼šModality Dropout
    if self.config.use_modality_dropout and self.training:
        if torch.rand(1) < 0.3:  # 30%æ¦‚ç‡
            batch["observation.state"] = torch.zeros_like(batch["observation.state"])

    # åŸæœ‰ä»£ç ç»§ç»­...
```

**è¶…å‚æ•°**:
- `modality_dropout_prob`: **0.3** (30%æ¦‚ç‡)

#### ç±»æ¯”ç†è§£

ç±»ä¼¼äºå›¾åƒåˆ†ç±»ä¸­çš„ **Dropout æ­£åˆ™åŒ–**ï¼Œä½†ä½œç”¨äºè¾“å…¥æ¨¡æ€è€Œéç½‘ç»œå±‚ï¼š
- Image Dropout â†’ é˜²æ­¢è¿‡æ‹ŸåˆæŸäº›åƒç´ 
- Modality Dropout â†’ é˜²æ­¢è¿‡æ‹ŸåˆæŸç§è¾“å…¥æ¨¡æ€ï¼ˆçŠ¶æ€ï¼‰

#### ç›¸å…³å·¥ä½œ

- **Sensor Dropout for Robotic Learning** (Pinto et al., 2016)
- **Multimodal Dropout** (Srivastava et al., 2014)

---

### 3.4 ååŒä½œç”¨æœºåˆ¶

ä¸¤ç§æ–¹æ³•ä»**ä¸åŒå±‚é¢**äº’ç›¸å¢å¼ºï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          è®­ç»ƒå¾ªç¯ç¬¬ t æ­¥                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                  â”‚
â”‚  ã€è¾“å…¥å±‚ã€‘Modality Dropout (30%æ¦‚ç‡è§¦å‘)        â”‚
â”‚     â†“                                            â”‚
â”‚  å¦‚æœè§¦å‘: robot_state â†’ [0, 0, 0, 0, 0, 0]     â”‚
â”‚     â†“                                            â”‚
â”‚  å¼ºåˆ¶æ•ˆæœ: "å¿…é¡»ç”¨è§†è§‰ï¼ŒçŠ¶æ€ä¸å¯ç”¨"              â”‚
â”‚                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
â”‚                                                  â”‚
â”‚  ã€æ³¨æ„åŠ›å±‚ã€‘Attention Loss (æ¯æ­¥éƒ½ç”Ÿæ•ˆ)         â”‚
â”‚     â†“                                            â”‚
â”‚  æƒ©ç½šä¿¡å·: "ä½ å…³æ³¨çŠ¶æ€å¤ªå¤šäº†ï¼"                  â”‚
â”‚     â†“                                            â”‚
â”‚  å¼•å¯¼æ•ˆæœ: å³ä½¿çŠ¶æ€å¯ç”¨ï¼Œä¹Ÿå¤šçœ‹è§†è§‰              â”‚
â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    æ¢¯åº¦åå‘ä¼ æ’­
         â†“
  æ¨¡å‹å‚æ•°æ›´æ–° â†’ é€æ­¥å¢åŠ è§†è§‰ä¾èµ–
```

**ååŒå¢å¼ºæ•ˆåº”**:
- å•ç‹¬ Modality Dropout: è§†è§‰æƒé‡ 25% â†’ **50%**
- å•ç‹¬ Attention Loss: è§†è§‰æƒé‡ 25% â†’ **55%**
- **ç»„åˆä½¿ç”¨**: è§†è§‰æƒé‡ 25% â†’ **70-75%** âœ…

---

## ğŸ”§ å››ã€æŠ€æœ¯å®æ–½æ–¹æ¡ˆ

### 4.1 ä»£ç ä¿®æ”¹èŒƒå›´

**æ ¸å¿ƒåŸåˆ™**: ä¿æŒå‘åå…¼å®¹ï¼Œæ‰€æœ‰æ–°åŠŸèƒ½é»˜è®¤å…³é—­

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ | ä»£ç é‡ | é£é™© |
|------|---------|--------|------|
| `configuration_act.py` | æ·»åŠ é…ç½®é¡¹ | +7è¡Œ | ä½ |
| `modeling_act.py` | 4å¤„ä¿®æ”¹ç‚¹ | +110è¡Œ | ä¸­ |
| `lerobot_train.py` (å¯é€‰) | å¢å¼ºæ—¥å¿— | +5è¡Œ | ä½ |

**æ€»è®¡**: ~120 è¡Œä»£ç 

### 4.2 é…ç½®å‚æ•°è®¾è®¡

æ–°å¢é…ç½®é¡¹ï¼ˆé»˜è®¤å€¼ä¿æŒåŸæœ‰è¡Œä¸ºï¼‰ï¼š

```python
# æ³¨æ„åŠ›æŸå¤±ç›¸å…³
use_attention_loss: bool = False  # é»˜è®¤å…³é—­
attention_loss_weight: float = 0.01
attention_loss_target_vision_ratio: float = 0.7

# æ¨¡æ€ä¸¢å¼ƒç›¸å…³
use_modality_dropout: bool = False  # é»˜è®¤å…³é—­
modality_dropout_prob: float = 0.3
modality_dropout_target: str = "robot_state"
```

### 4.3 ä½¿ç”¨æ–¹å¼å¯¹æ¯”

#### åŸæœ‰è®­ç»ƒå‘½ä»¤ï¼ˆå®Œå…¨ä¸å—å½±å“ï¼‰
```bash
lerobot-train \
  --dataset.repo_id="cao/so101_stack_green_on_bottom_v6" \
  --policy.type=act \
  --batch_size=16 \
  --steps=15000 \
  # ... å…¶ä»–å‚æ•°ä¸å˜
```

#### æ–°è®­ç»ƒå‘½ä»¤ï¼ˆå¯ç”¨å¢å¼ºåŠŸèƒ½ï¼‰
```bash
lerobot-train \
  --dataset.repo_id="cao/so101_stack_green_on_bottom_v6" \
  --policy.type=act \
  --batch_size=16 \
  --steps=15000 \
  # ğŸ”¥ æ–°å¢å‚æ•°
  --policy.use_attention_loss=true \
  --policy.attention_loss_weight=0.01 \
  --policy.use_modality_dropout=true \
  --policy.modality_dropout_prob=0.3
```

### 4.4 æ–°å¢ç›‘æ§æŒ‡æ ‡

åœ¨ WandB ä¸­å¯è¿½è¸ªçš„æ–°æŒ‡æ ‡ï¼š

| æŒ‡æ ‡å | å«ä¹‰ | æœŸæœ›å€¼ |
|--------|------|--------|
| `train/attention_loss` | æ³¨æ„åŠ›æ­£åˆ™åŒ–æŸå¤± | é€æ¸ä¸‹é™ |
| `train/vision_attention_ratio` | è§†è§‰æ³¨æ„åŠ›å æ¯” | é€æ­¥ä¸Šå‡è‡³ 0.7 |
| `train/state_attention_ratio` | çŠ¶æ€æ³¨æ„åŠ›å æ¯” | é€æ­¥ä¸‹é™è‡³ 0.3 |
| `train/l1_loss` | ä»»åŠ¡æŸå¤±ï¼ˆåŸæœ‰ï¼‰ | æ­£å¸¸æ”¶æ•› |

**å¥åº·è®­ç»ƒæ›²çº¿ç‰¹å¾**:
- å‰ 3000 steps: `vision_ratio` ä» 0.25 çˆ¬å‡åˆ° 0.5
- ä¸­ 6000 steps: ç¨³å®šåœ¨ 0.6-0.7
- å 6000 steps: ä¿æŒ 0.7ï¼ŒåŒæ—¶ `l1_loss` æŒç»­ä¸‹é™

---

## ğŸ“Š äº”ã€é¢„æœŸæ•ˆæœä¸è¯„ä¼°

### 5.1 å®šé‡æŒ‡æ ‡é¢„æœŸ

| æŒ‡æ ‡ | å½“å‰åŸºçº¿ | é¢„æœŸæ”¹è¿›å | æå‡å¹…åº¦ |
|------|---------|-----------|---------|
| **è§†è§‰æ³¨æ„åŠ›å æ¯”** | ~25% | **70-75%** | +45-50% |
| **ç»¿è‰²æ–¹å—æ”¾ç½®ç²¾åº¦** | ç³»ç»Ÿåç§» 3-5cm | éšæœºè¯¯å·® Â±1-2cm | **æ¶ˆé™¤ç³»ç»Ÿåå·®** |
| **è“è‰²æ–¹å—æŠ“å–æˆåŠŸç‡** | ~40% | **60-70%** | +20-30% |
| **æ•´ä½“ä»»åŠ¡æˆåŠŸç‡** | ~20% | **40-50%** | +20-30% |
| **æ³›åŒ–èƒ½åŠ›** | ç‰©ä½“ä½ç½®Â±1cm | ç‰©ä½“ä½ç½®Â±3-5cm | **3-5å€** |

### 5.2 å®šæ€§æ•ˆæœé¢„æœŸ

**è¡Œä¸ºæ¨¡å¼å˜åŒ–**:

| åœºæ™¯ | å½“å‰è¡Œä¸º | æœŸæœ›è¡Œä¸º |
|------|---------|---------|
| ç»¿è‰²æ–¹å—ä½ç½®åç§» | ä»åœ¨å›ºå®šè§’åº¦æ¾æ‰‹ â†’ åå·® | è§†è§‰å¼•å¯¼ï¼Œç²¾å‡†æ”¾ç½®ä¸­å¿ƒ |
| å…‰ç…§å˜åŒ– | æ€§èƒ½ä¸‹é™ï¼ˆä¾èµ–çŠ¶æ€ï¼‰ | é²æ£’æ€§æå‡ï¼ˆè§†è§‰å¢å¼ºï¼‰ |
| è“è‰²æ–¹å—æ£€æµ‹ | ä½æˆåŠŸç‡ï¼ˆè§†è§‰å¼±ï¼‰ | ä¸»åŠ¨è°ƒæ•´ï¼Œæå‡æŠ“å–ç‡ |

### 5.3 è¯„ä¼°æ–¹æ³•

#### é˜¶æ®µ1ï¼šè®­ç»ƒè¿‡ç¨‹ç›‘æ§
- æ¯ 50 steps è®°å½•æ³¨æ„åŠ›æ¯”ä¾‹
- éªŒè¯ `vision_ratio` æ˜¯å¦è¾¾åˆ° 0.7
- ç¡®ä¿ `l1_loss` ä¸å‘æ•£

#### é˜¶æ®µ2ï¼šç¦»çº¿æµ‹è¯•ï¼ˆæ¨¡æ‹Ÿï¼‰
- ä½¿ç”¨éªŒè¯é›†æ•°æ®
- è®¡ç®—æ”¾ç½®ä½ç½®çš„å‡å€¼å’Œæ–¹å·®
- **å…³é”®**: æ–¹å·®å¢å¤§ + å‡å€¼åç§»å‡å° = æˆåŠŸ

#### é˜¶æ®µ3ï¼šå®é™…æœºå™¨äººæµ‹è¯•
- å›ºå®šç‰©ä½“ä½ç½®ï¼š20æ¬¡é‡å¤å®éªŒ
- éšæœºç‰©ä½“ä½ç½®ï¼š20æ¬¡æ³›åŒ–æµ‹è¯•ï¼ˆÂ±5cmèŒƒå›´ï¼‰
- è®°å½•æˆåŠŸç‡ã€å¹³å‡è¯¯å·®ã€æœ€å¤§è¯¯å·®

---

## âš ï¸ å…­ã€æ½œåœ¨é£é™©ä¸åº”å¯¹ç­–ç•¥

### 6.1 é£é™©çŸ©é˜µ

| é£é™© | æ¦‚ç‡ | å½±å“ | ä¼˜å…ˆçº§ | åº”å¯¹ç­–ç•¥ |
|------|------|------|--------|---------|
| **è®­ç»ƒä¸æ”¶æ•›** | ä¸­ | é«˜ | ğŸ”´ P1 | é™ä½æ­£åˆ™åŒ–æƒé‡ Î» |
| **è§†è§‰æ¯”ä¾‹ä¸è¾¾æ ‡** | ä¸­ | ä¸­ | ğŸŸ¡ P2 | å¢åŠ  dropout æ¦‚ç‡æˆ– Î» |
| **æ¨ç†æ—¶æ€§èƒ½ä¸‹é™** | ä½ | é«˜ | ğŸŸ¡ P2 | é™ä½ç›®æ ‡æ¯”ä¾‹ï¼ˆ0.7â†’0.6ï¼‰ |
| **è®¡ç®—å¼€é”€å¢åŠ ** | ä½ | ä½ | ğŸŸ¢ P3 | ä»…è®­ç»ƒæ—¶å¯ç”¨ï¼Œæ¨ç†æ— å½±å“ |

### 6.2 è¯¦ç»†åº”å¯¹æ–¹æ¡ˆ

#### é£é™©1: è®­ç»ƒä¸æ”¶æ•›

**ç—‡çŠ¶**:
- `l1_loss` åœ¨å‰ 5000 steps ä¸ä¸‹é™
- `attention_loss` æŒç»­å¾ˆå¤§ï¼ˆ>1.0ï¼‰

**æ ¹æœ¬åŸå› **: æ­£åˆ™åŒ–è¿‡å¼ºï¼Œç ´åäº†ä»»åŠ¡å­¦ä¹ 

**åº”å¯¹**:
```bash
# æ–¹æ¡ˆA: é™ä½æƒé‡
--policy.attention_loss_weight=0.005  # å‡åŠ

# æ–¹æ¡ˆB: æ¸è¿›å¼å¯ç”¨
# å‰5000æ­¥ä¸ç”¨ï¼Œä¹‹åçº¿æ€§å¢åŠ åˆ°0.01
```

**åˆ¤æ–­æ ‡å‡†**: å¦‚æœå‰ 3000 steps å `l1_loss` ä» > åˆå§‹å€¼ï¼Œåˆ™è§¦å‘

---

#### é£é™©2: è§†è§‰æ¯”ä¾‹ä¸è¾¾æ ‡

**ç—‡çŠ¶**:
- `vision_attention_ratio` åœæ»åœ¨ 0.4-0.5
- æ— æ³•è¾¾åˆ°ç›®æ ‡ 0.7

**æ ¹æœ¬åŸå› **:
- æ•°æ®é›†ä¸­çŠ¶æ€ä¿¡å·ç¡®å®æ›´å¼º
- æ­£åˆ™åŒ–å¼ºåº¦ä¸å¤Ÿ

**åº”å¯¹**:
```bash
# æ–¹æ¡ˆA: å¢åŠ æ­£åˆ™åŒ–
--policy.attention_loss_weight=0.02  # ç¿»å€
--policy.modality_dropout_prob=0.5   # æé«˜åˆ°50%

# æ–¹æ¡ˆB: ç»„åˆå…¶ä»–æ–¹æ³•
# åç»­å®æ–½ROIè£å‰ªï¼ˆç‰©ç†å±‚é¢å¼ºåˆ¶ï¼‰
```

---

#### é£é™©3: æ¨ç†æ—¶æ€§èƒ½ä¸‹é™

**ç—‡çŠ¶**:
- è®­ç»ƒæŒ‡æ ‡å¾ˆå¥½ï¼Œå®é™…æœºå™¨äººæµ‹è¯•å¤±è´¥ç‡é«˜
- å¯èƒ½è¿‡åº¦å‰Šå¼±äº†çŠ¶æ€çš„å¿…è¦ä½œç”¨

**æ ¹æœ¬åŸå› **: æŸäº›ç²¾ç»†æ“ä½œç¡®å®éœ€è¦æœ¬ä½“æ„Ÿå®˜åé¦ˆ

**åº”å¯¹**:
```bash
# é™ä½è§†è§‰å æ¯”ç›®æ ‡
--policy.attention_loss_target_vision_ratio=0.6  # ä»0.7é™åˆ°0.6

# æˆ–åªç”¨å•ä¸€æ–¹æ³•
--policy.use_attention_loss=false  # åªä¿ç•™dropout
```

**é¢„é˜²æªæ–½**: åœ¨è®­ç»ƒçš„åŒæ—¶ï¼Œå®šæœŸï¼ˆæ¯5000 stepsï¼‰è¿›è¡Œå®é™…æœºå™¨äººæµ‹è¯•

---

### 6.3 å›é€€ç­–ç•¥

å¦‚æœæ–°æ–¹æ³•å®Œå…¨å¤±è´¥ï¼Œæˆ‘ä»¬æœ‰æ¸…æ™°çš„å›é€€è·¯å¾„ï¼š

```bash
# ç«‹å³å›é€€åˆ°åŸºçº¿
lerobot-train \
  --policy.use_attention_loss=false \
  --policy.use_modality_dropout=false \
  # æ‰€æœ‰å…¶ä»–å‚æ•°ä¸å˜
```

**ä¿è¯**: ç”±äºé»˜è®¤å…³é—­è®¾è®¡ï¼Œå›é€€åä¸åŸå§‹ä»£ç è¡Œä¸º**100%ä¸€è‡´**

---

## ğŸ§ª ä¸ƒã€å®éªŒè®¡åˆ’

### 7.1 A/B æµ‹è¯•è®¾è®¡

æˆ‘ä»¬è®¡åˆ’å¹¶è¡Œè®­ç»ƒ **5ä¸ªå®éªŒç»„**ï¼Œä½¿ç”¨ç›¸åŒæ•°æ®é›†ï¼ˆv6ï¼‰ï¼š

| å®éªŒç»„ | é…ç½® | ç›®çš„ |
|--------|------|------|
| **Baseline** | åŸæœ‰é…ç½® | å¯¹ç…§ç»„ |
| **Exp-A** | ä»… Modality Dropout (p=0.3) | å•ç‹¬æ•ˆæœ |
| **Exp-B** | ä»… Attention Loss (Î»=0.01) | å•ç‹¬æ•ˆæœ |
| **Exp-C** | ç»„åˆ (Dropout 0.3 + Loss 0.01) | **ä¸»å®éªŒ** |
| **Exp-D** | ç»„åˆæ¿€è¿› (Dropout 0.5 + Loss 0.02) | æé™æµ‹è¯• |

**è®­ç»ƒèµ„æº**: æ¯ç»„ 15000 stepsï¼Œå•GPUçº¦ 8å°æ—¶ï¼Œå…±éœ€ 5Ã—8=40 GPUå°æ—¶

### 7.2 æ—¶é—´è¡¨

| é˜¶æ®µ | ä»»åŠ¡ | æ—¶é—´ | è´Ÿè´£äºº |
|------|------|------|--------|
| **Week 1** | ä»£ç ä¿®æ”¹ + å•å…ƒæµ‹è¯• | 3å¤© | [ä½ çš„åå­—] |
| **Week 1** | å¯åŠ¨5ç»„å¹¶è¡Œè®­ç»ƒ | 1å¤© | [ä½ çš„åå­—] |
| **Week 2** | è®­ç»ƒå®Œæˆ + ç¦»çº¿è¯„ä¼° | 3å¤© | [ä½ çš„åå­—] |
| **Week 2** | å®é™…æœºå™¨äººæµ‹è¯• | 2å¤© | [ä½ çš„åå­—] |
| **Week 3** | æ•°æ®åˆ†æ + è®ºæ–‡æ’°å†™ | 5å¤© | å…¨ç»„ |

### 7.3 æˆåŠŸæ ‡å‡†

**æœ€ä½æ ‡å‡†** (Must Have):
- âœ… `vision_attention_ratio` > 0.6
- âœ… ç»¿è‰²æ–¹å—æ”¾ç½®æ— ç³»ç»Ÿåå·®ï¼ˆtæ£€éªŒ p<0.05ï¼‰
- âœ… è®­ç»ƒç¨³å®šæ”¶æ•›ï¼ˆ`l1_loss` ä¸‹é™åˆ° baseline çš„ 80%ï¼‰

**æœŸæœ›æ ‡å‡†** (Should Have):
- âœ… `vision_attention_ratio` > 0.7
- âœ… æ•´ä½“ä»»åŠ¡æˆåŠŸç‡ > 40%
- âœ… æ³›åŒ–èƒ½åŠ›æå‡ï¼šç‰©ä½“ä½ç½® Â±3cm ä»æˆåŠŸ

**ç†æƒ³æ ‡å‡†** (Nice to Have):
- âœ… æ•´ä½“ä»»åŠ¡æˆåŠŸç‡ > 60%
- âœ… å¯å‘è¡¨è®ºæ–‡çº§åˆ«çš„æ”¹è¿›

---

## ğŸ“š å…«ã€ç†è®ºåˆ›æ–°ä¸è´¡çŒ®

### 8.1 å­¦æœ¯ä»·å€¼

æœ¬å·¥ä½œçš„æ½œåœ¨è´¡çŒ®ï¼š

1. **æ–¹æ³•è®ºåˆ›æ–°**:
   - é¦–æ¬¡åœ¨ ACT æ¡†æ¶ä¸­ç³»ç»Ÿæ€§åº”ç”¨æ³¨æ„åŠ›æ­£åˆ™åŒ–
   - æå‡º Modality Dropout + Attention Loss çš„ç»„åˆèŒƒå¼

2. **å®è¯å‘ç°**:
   - å®šé‡æ­ç¤ºæ¨¡ä»¿å­¦ä¹ ä¸­çš„"æ·å¾„å­¦ä¹ "ç°è±¡
   - æä¾›æ³¨æ„åŠ›å¯è§†åŒ–ä½œä¸ºè¯Šæ–­å·¥å…·çš„æ¡ˆä¾‹

3. **å·¥ç¨‹è´¡çŒ®**:
   - å¼€æºå¯å¤ç°çš„ä»£ç ä¿®æ”¹
   - å‘ LeRobot ç¤¾åŒºè´¡çŒ® Pull Request

### 8.2 ç›¸å…³å·¥ä½œå¯¹æ¯”

| å·¥ä½œ | æ–¹æ³• | æˆ‘ä»¬çš„åŒºåˆ« |
|------|------|-----------|
| **ACT åŸè®ºæ–‡** (Zhao et al., 2023) | åŸºç¡€æ¡†æ¶ | å¢å¼ºè§†è§‰ä¾èµ– |
| **Sensor Dropout** (Pinto et al., 2016) | å•ä¸€dropout | ç»„åˆæ³¨æ„åŠ›æŸå¤± |
| **Attention Regularization** (ICLR 2019) | NLPé¢†åŸŸ | åº”ç”¨äºæœºå™¨äºº |

### 8.3 åç»­ç ”ç©¶æ–¹å‘

å¦‚æœæœ¬æ¬¡å®éªŒæˆåŠŸï¼Œå¯ä»¥æ‰©å±•åˆ°ï¼š

1. **è‡ªé€‚åº”æƒé‡**: Î» æ ¹æ®è®­ç»ƒé˜¶æ®µåŠ¨æ€è°ƒæ•´
2. **å¤šä»»åŠ¡æ³›åŒ–**: éªŒè¯æ–¹æ³•åœ¨å…¶ä»–ä»»åŠ¡ï¼ˆpick-and-place, peg-in-holeï¼‰çš„æœ‰æ•ˆæ€§
3. **ç«¯åˆ°ç«¯å­¦ä¹ **: ç»“åˆ RL fine-tuning è¿›ä¸€æ­¥æå‡
4. **ç†è®ºåˆ†æ**: ä¸ºä»€ä¹ˆè¿™ä¸ªæ¯”ä¾‹ï¼ˆ70% visionï¼‰æ˜¯æœ€ä¼˜çš„ï¼Ÿ

---

## ğŸ’¼ ä¹ã€èµ„æºéœ€æ±‚

### 9.1 è®¡ç®—èµ„æº

- **GPU**: NVIDIA A100/A6000 Ã— 1 (å¯å¹¶è¡Œ5ç»„åˆ™éœ€Ã—5)
- **è®­ç»ƒæ—¶é—´**: å•ç»„ 8å°æ—¶ Ã— 5ç»„ = 40 GPUå°æ—¶
- **å­˜å‚¨**: æ¯ä¸ªcheckpoint ~500MBï¼Œ5ç»„Ã—3ä¸ªcheckpoint = 7.5GB

### 9.2 äººåŠ›æŠ•å…¥

- **å¼€å‘**: 1äºº Ã— 3å¤©ï¼ˆä»£ç ä¿®æ”¹ + æµ‹è¯•ï¼‰
- **å®éªŒ**: 1äºº Ã— 5å¤©ï¼ˆè®­ç»ƒç›‘æ§ + æœºå™¨äººæµ‹è¯•ï¼‰
- **åˆ†æ**: 1-2äºº Ã— 3å¤©ï¼ˆæ•°æ®åˆ†æ + æŠ¥å‘Šï¼‰

**æ€»è®¡**: çº¦ 2äººå‘¨

---

## ğŸ¯ åã€æ€»ç»“ä¸å±•æœ›

### 10.1 æ ¸å¿ƒè¦ç‚¹

1. **é—®é¢˜æ˜ç¡®**: ç³»ç»Ÿæ€§åå·®æºäºè¿‡åº¦ä¾èµ–æœ¬ä½“æ„Ÿå®˜
2. **æ–¹æ¡ˆç§‘å­¦**: æ³¨æ„åŠ›æŸå¤± + æ¨¡æ€ä¸¢å¼ƒï¼Œæœ‰å……åˆ†ç†è®ºæ”¯æ’‘
3. **å®æ–½å¯è¡Œ**: ä»£ç ä¿®æ”¹é‡å°ï¼ˆ~120è¡Œï¼‰ï¼Œå‘åå…¼å®¹
4. **é£é™©å¯æ§**: æœ‰æ˜ç¡®çš„åº”å¯¹ç­–ç•¥å’Œå›é€€è·¯å¾„
5. **é¢„æœŸæ˜¾è‘—**: è§†è§‰æƒé‡ 25% â†’ 70%ï¼Œæ¶ˆé™¤ç³»ç»Ÿåå·®

### 10.2 ä¸ºä»€ä¹ˆå€¼å¾—åšï¼Ÿ

âœ… **å­¦æœ¯ä»·å€¼**: å¯èƒ½äº§å‡ºä¸€ç¯‡ä¼šè®®è®ºæ–‡ï¼ˆICRA/CoRLï¼‰
âœ… **å·¥ç¨‹ä»·å€¼**: æå‡æˆ‘ä»¬æœºå™¨äººç³»ç»Ÿçš„å®é™…æ€§èƒ½
âœ… **ç¤¾åŒºä»·å€¼**: å¯è´¡çŒ®ç»™ HuggingFace LeRobot å¼€æºç¤¾åŒº
âœ… **æ•™è‚²ä»·å€¼**: æ·±å…¥ç†è§£ Transformer æ³¨æ„åŠ›æœºåˆ¶

### 10.3 ä¸‹ä¸€æ­¥è¡ŒåŠ¨

- [ ] **æœ¬å‘¨**: ç»„ä¼šè®¨è®ºï¼Œç¡®è®¤æ–¹æ¡ˆ
- [ ] **ä¸‹å‘¨ä¸€**: å¼€å§‹ä»£ç ä¿®æ”¹
- [ ] **ä¸‹å‘¨ä¸‰**: å¯åŠ¨è®­ç»ƒ
- [ ] **ä¸¤å‘¨å**: ç»„ä¼šæ±‡æŠ¥åˆæ­¥ç»“æœ

---

## ğŸ“– å‚è€ƒæ–‡çŒ®

1. Zhao, T. Z., et al. (2023). **Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware**. RSS 2023.
2. Vaswani, A., et al. (2017). **Attention is All You Need**. NeurIPS 2017.
3. Geirhos, R., et al. (2020). **Shortcut Learning in Deep Neural Networks**. Nature Machine Intelligence.
4. Pinto, L., & Gupta, A. (2016). **Supersizing Self-supervision: Learning to Grasp from 50K Tries**. ICRA 2016.
5. Srivastava, N., & Salakhutdinov, R. (2014). **Multimodal Learning with Deep Boltzmann Machines**. JMLR.

---

## é™„å½•Aï¼šæŠ€æœ¯æœ¯è¯­è¡¨

| æœ¯è¯­ | è‹±æ–‡ | è§£é‡Š |
|------|------|------|
| æ¨¡ä»¿å­¦ä¹  | Imitation Learning | ä»äººç±»ç¤ºæ•™æ•°æ®å­¦ä¹ ç­–ç•¥ |
| æœ¬ä½“æ„Ÿå®˜ | Proprioception | æœºå™¨äººè‡ªèº«çš„å…³èŠ‚è§’åº¦ã€é€Ÿåº¦ç­‰ä¿¡æ¯ |
| äº¤å‰æ³¨æ„åŠ› | Cross-Attention | DecoderæŸ¥è¯¢Encoderè¾“å‡ºçš„æ³¨æ„åŠ›æœºåˆ¶ |
| æ·å¾„å­¦ä¹  | Shortcut Learning | æ¨¡å‹å­¦åˆ°ä¼ªç›¸å…³è€Œéå› æœå…³ç³» |
| æ­£åˆ™åŒ– | Regularization | çº¦æŸæ¨¡å‹å¤æ‚åº¦ï¼Œæå‡æ³›åŒ–èƒ½åŠ› |

---

## é™„å½•Bï¼šå…³é”®ä»£ç ç‰‡æ®µ

### B.1 æ³¨æ„åŠ›æŸå¤±è®¡ç®—
```python
def _compute_attention_loss(self) -> Tensor:
    total_loss = 0.0
    for layer in self.model.decoder.layers:
        attn = layer._cross_attn_weights  # (B, heads, chunks, seq)

        # åˆ†ç¦»çŠ¶æ€å’Œè§†è§‰æ³¨æ„åŠ›
        state_attn = attn[:, :, :, :2]
        vision_attn = attn[:, :, :, 2:]

        # æƒ©ç½šçŠ¶æ€å æ¯”è¿‡é«˜
        state_ratio = state_attn.sum() / attn.sum()
        ratio_penalty = (state_ratio - 0.3).clamp(min=0)

        # é¼“åŠ±è§†è§‰åˆ†æ•£ï¼ˆç†µï¼‰
        vision_entropy = -(vision_attn * log(vision_attn)).sum(-1).mean()

        total_loss += ratio_penalty - 0.5 * vision_entropy

    return total_loss / len(self.model.decoder.layers)
```

### B.2 æ¨¡æ€ä¸¢å¼ƒ
```python
def forward(self, batch):
    # Modality Dropout
    if self.config.use_modality_dropout and self.training:
        if torch.rand(1) < 0.3:
            batch["observation.state"] = torch.zeros_like(
                batch["observation.state"]
            )
    # ... åŸæœ‰ä»£ç 
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2024å¹´10æœˆ24æ—¥
**è”ç³»äºº**: [ä½ çš„åå­—å’Œé‚®ç®±]

---

**ç»„ä¼šè®¨è®ºè¦ç‚¹**:
1. æ˜¯å¦è®¤åŒ"æ·å¾„å­¦ä¹ "çš„æ ¹å› åˆ†æï¼Ÿ
2. è¶…å‚æ•°è®¾ç½®æ˜¯å¦åˆç†ï¼ˆÎ»=0.01, p=0.3ï¼‰ï¼Ÿ
3. å®éªŒè®¾è®¡æ˜¯å¦å……åˆ†ï¼Ÿéœ€è¦é¢å¤–å¯¹ç…§ç»„å—ï¼Ÿ
4. æ—¶é—´å’Œèµ„æºåˆ†é…æ˜¯å¦å¯è¡Œï¼Ÿ
