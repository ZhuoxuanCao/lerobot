# 本周工作

## **so101的模仿学习：更新黑框-优化遥操作**

- 原版的放置点是使用的纯色黑框，现在在黑框的基础上加上红色的标记线

  ![image-20251024121600056](C:\Users\MECHREUO\AppData\Roaming\Typora\typora-user-images\image-20251024121600056.png)

- 在遥操作时，加入：**悬停-校正-放置”三段式操作**

### 测试及问题

**样本1：**

- 训练集：将右边绿色方块，放在左边的**黑色红色**框里20次，并且加入三段式操作
- 训练细节：
  - 减少epoch数，不再追求极致loss
  - 加入数据增强（亮度、模糊等，HF自带）

- 准确率：相比于之前有提升，目前准确率大约为50%（将方块大致放进框里），但有时依然放不进去
- 问题：
  - offset的问题依然存在
  - 模型过拟合向电机的问题依然存在

在该样本中，我们尝试**移除了黑红框**，它依然会将绿色方块放过去，但是相比之前更加犹豫

说明我们的优化方向是正确的（加强视觉信号特征+减缓机械臂移动速度），可以进一步往这个方向努力。



### 优化思路&算法底层的修改

- ~~能不能做一个推方块的policy？~~，遥操作困难，难以实现高精度的推箱子
- ~~拆分policy，先做一个抓取policy，然后再做一个放置policy~~，hold暂时不可行，未找到电机不断电的办法
- ~~为不同的遥操作数据引入评分系统，给高质量操作高分，低质量操作低分。~~遥操作-数据集的合成-训练，的代码已经耦合，加入这个功能需要解耦，工程量大，暂时先不考虑。
- 引入一些机制让模型注意力更多在视觉上，减小电机信号的权重，增加视觉信号的权重（**正在尝试，还没跑通**）：
  - 在训练过程中，随机（概率 p=0.3）将机器人状态 `robot_state` 置零，即参考dropout的思路，训练时随机丢弃一些电机数据。希望只能依赖视觉来预测动作
  - loss函数的优化，当前已有L1 + KLD两个loss，其中L1 面向“输出动作”的精确回归；KLD 面向“潜变量分布”的结构正则。L1 提升贴合示教轨迹的准确度；KLD 提升分布合理性与**泛化/可采样性**。我们怀疑：尽管我们当前已经使用了两个loss，但是KLD的效果不佳，或者说被L1压制。
    - 尝试提升KLD loss的权重，增加kl_weight，从10-15-20
    - 尝试控制L1 loss的权重

### 尝试控制L1 loss的权重

在原有的任务损失（L1 loss）和变分正则（KLD loss）基础上，添加一个**注意力正则项**来引导模型的注意力分布：

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{L1}} + \text{kl\_weight} \cdot \mathcal{L}_{\text{KLD}} + \lambda \cdot \mathcal{L}_{\text{attention}}
$$

**参数说明**：

- **总损失** $\mathcal{L}_{\text{total}}$：最终的总损失，用于反向传播更新模型参数
- **任务损失** $\mathcal{L}_{\text{L1}}$：原有的任务损失，衡量预测动作与真实动作的差距
- **变分正则损失** $\mathcal{L}_{\text{KLD}}$：当启用 VAE（默认开启）时，引导潜变量分布接近标准正态，以提升可采样性与泛化
- **权重系数** $\lambda$：平衡任务学习和注意力正则化的超参数
- **注意力正则化损失** $\mathcal{L}_{\text{attention}}$：下文详细展开



## 下一步工作？

继续研究IL？

seminar？

